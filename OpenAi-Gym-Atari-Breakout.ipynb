{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee73b59",
   "metadata": {},
   "source": [
    "# OpenAI Gym\n",
    "[Gym](https://www.gymlibrary.dev/) is an open-source Python library, made for the easy development and testing of reinforcement learning algorithms. Today we will use it to create and run an instance of the Atari game Breakout. The Gym library provides us access to the game state, game rewards, and available actions, which if you remember are necessary parts of our RL framework. \n",
    "\n",
    "<img src=\"Media/Test.gif\" width=\"200\" align=\"center\">\n",
    "\n",
    "### The RL Framework for Breakout:\n",
    "- **Action:** Move the paddle left and right \n",
    "- **State:** The 210x160 RGB image frame \n",
    "- **Reward:** Amount the game score increases \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773c08b",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf8785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import cv2\n",
    "import time \n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef34e1bd",
   "metadata": {},
   "source": [
    "We need to set the environment variable ALE_PY_ROM_DIR to the directory of the bins so that we can use the namespace ALE/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b10f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.0+919230b)\n",
      "[Powered by Stella]\n",
      "Game console created:\n",
      "  ROM file:  /Users/justinvalentine/opt/anaconda3/envs/Test/lib/python3.9/site-packages/ale_py/roms/breakout.bin\n",
      "  Cart Name: Breakout - Breakaway IV (1978) (Atari)\n",
      "  Cart MD5:  f34f08e5eb96e500e851a80be3277a56\n",
      "  Display Format:  AUTO-DETECT ==> NTSC\n",
      "  ROM Size:        2048\n",
      "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
      "\n",
      "Running ROM file...\n",
      "Random seed is 1667373141\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()\n",
    "\n",
    "from ale_py.roms import Breakout\n",
    "ale.loadROM(Breakout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b2496",
   "metadata": {},
   "source": [
    "## Creating the Atari Breakout Instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01518217",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Breakout-v5\") # creats a game instance of Atari Breakout \n",
    "\n",
    "env.reset()\n",
    "image_lst = []\n",
    "\n",
    "step_num, total_reward = 0, 0\n",
    "\n",
    "while step_num < 100:\n",
    "    # Used to generates random actions\n",
    "    action = env.env.action_space.sample()\n",
    "    \n",
    "    # Get the next state, and reward after taking your action \n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Create image frame \n",
    "    img = cv2.resize(state, (160, 210), interpolation=cv2.INTER_CUBIC)\n",
    "    frame = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    image_lst.append(frame)\n",
    "        \n",
    "    time.sleep(0.01)\n",
    "    step_num += 1\n",
    "\n",
    "env.close()\n",
    "\n",
    "imageio.mimsave('Media/random-action.gif', image_lst, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a9b63",
   "metadata": {},
   "source": [
    "<img src=\"Media/random-action.gif\" width=\"300\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751bd81",
   "metadata": {},
   "source": [
    "## Applying Reinforcement Learning to Breakout \n",
    "Recall that in Reinforcement Learning then the main goal of our agent is to maximize its expected future reward. This means that at each time step the agent wants to try and select the best action given its current state at that time step. \n",
    "\n",
    "However it is not enough for the agent to just select the greedy action at every time-step, the agent must also explore its other options. The agent then 'learns' what actions are good in what states, and 'remembers' these results.\n",
    "\n",
    "So how can we train the agent to play like a human, when all the agent can do is move the paddle and watch what happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc67205",
   "metadata": {},
   "source": [
    "### The Challenges\n",
    "The first challenge we need to address is that each state of the game contains a lot of information! In this case, each frame of the game is a 210x160 RGB image! That means that the state of the game on any time step will be encoded by 210x160=33600 pixel values... This is to much. But how can we reduce the amount of total information, while keeping important features present?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf82bd36",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning\n",
    "Well, we can look the the 2013 paper by DeepMind [here](https://www.deepmind.com/publications/playing-atari-with-deep-reinforcement-learning), their solution was to use neural networks to encode state information.\n",
    "\n",
    "Our game instance looks good and humans could train based off of it, but it is not suitable for the neural network so we will need to modify it. \n",
    "\n",
    "**Steps:**\n",
    "1. Rescale to the frame to an 84x84 grayscale image (already close to a 15x reduction in state data)\n",
    "2. We need to also encode the game dynamics because a static image is not enough for the NN to know the direction of the ball. We can encode the dynamics into the game frame by overlaying 4 successive frames \n",
    "\n",
    "The full code used to make the environment optimized for training, is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6331d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
    "env = NoopResetEnv(env, noop_max=30)\n",
    "env = MaxAndSkipEnv(env, skip=4)\n",
    "env = RecordEpisodeStatistics(env)\n",
    "env = EpisodicLifeEnv(env)\n",
    "env = FireResetEnv(env)\n",
    "env = WarpFrame(env)\n",
    "env = ClipRewardEnv(env)\n",
    "env = FrameStack(env, 4)\n",
    "env = ImageToPyTorch(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80582459",
   "metadata": {},
   "source": [
    "<img src=\"Media/Transformation.gif\" width=\"300\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d539a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5511c47aec90ff6d3bf2282aa911cd8b4e17a7dd8b958807807acb2e44bc0528"
  },
  "kernelspec": {
   "display_name": "Test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
