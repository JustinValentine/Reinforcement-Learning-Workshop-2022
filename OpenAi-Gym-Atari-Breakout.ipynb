{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc9ce69",
   "metadata": {},
   "source": [
    "# How to Design an Atari Breakout RL Agent ðŸ¤–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee73b59",
   "metadata": {},
   "source": [
    "## OpenAI Gym ðŸ’ª ðŸ§ \n",
    "[Gym](https://www.gymlibrary.dev/) is an open-source Python library, made for developing and comparing reinforcement learning algorithms. Gym provides us with an environment and then it is upto us to implement our reinforcement learning algorithms. Today we will use it to create and run an instance of the Atari game Breakout. The Gym library provides us access to the game state, game rewards, and available actions, which if you remember are a necessary parts of our RL framework. \n",
    "\n",
    "The code that we will be using to make our Deep RL agent can be found [here](https://github.com/dmitryelj/data-science-tutorials/blob/master/ai_breakout_game.py) It is based off of [this](https://wandb.ai/cleanrl/cleanrl.benchmark/runs/lqyi4g2g/files/code/cleanrl/ppo_atari_visual.py) demo which has some realy nice comparisons that can be seen [here](https://wandb.ai/cleanrl/cleanrl.benchmark/reports/Atari--VmlldzoxMTExNTI) \n",
    "\n",
    "<img src=\"Media/Test.gif\" width=\"200\" align=\"center\">\n",
    "\n",
    "### The RL Framework for Breakout:\n",
    "- **Action:** Move the paddle left and right \n",
    "- **State:** The 210x160 RGB image frame \n",
    "- **Reward:** Amount the game score increases \n",
    "\n",
    "Lets get started with Gym!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf8785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import cv2\n",
    "import time \n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef34e1bd",
   "metadata": {},
   "source": [
    "We need to set the environment variable ALE_PY_ROM_DIR to the directory of the bins so that we can use the namespace ALE/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b10f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.0+919230b)\n",
      "[Powered by Stella]\n",
      "Game console created:\n",
      "  ROM file:  /Users/justinvalentine/opt/anaconda3/envs/Test/lib/python3.9/site-packages/ale_py/roms/breakout.bin\n",
      "  Cart Name: Breakout - Breakaway IV (1978) (Atari)\n",
      "  Cart MD5:  f34f08e5eb96e500e851a80be3277a56\n",
      "  Display Format:  AUTO-DETECT ==> NTSC\n",
      "  ROM Size:        2048\n",
      "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
      "\n",
      "Running ROM file...\n",
      "Random seed is 1667547198\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()\n",
    "\n",
    "from ale_py.roms import Breakout\n",
    "ale.loadROM(Breakout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b2496",
   "metadata": {},
   "source": [
    "## Creating the Atari Game Instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01518217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.0+919230b)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "# create the Atari environment\n",
    "#env = gym.make(\"ALE/Breakout-v5\")\n",
    "\n",
    "# Try ME! (many more at https://www.gymlibrary.dev/environments/atari/complete_list/)\n",
    "#env = gym.make(\"ALE/Asteroids-v5\")\n",
    "#env = gym.make(\"ALE/MsPacman-v5\")\n",
    "#env = gym.make(\"ALE/SpaceInvaders-v5\")\n",
    "\n",
    "\n",
    "# list to store image frames (would not use if not in notebook)\n",
    "image_lst = []\n",
    "\n",
    "# run for 5 episodes\n",
    "for episode in range(5):\n",
    "    \n",
    "    # put the environment into its start state\n",
    "    env.reset() \n",
    "    \n",
    "    # run until the episode completes\n",
    "    terminated = False \n",
    "    \n",
    "    while not terminated:\n",
    "        \n",
    "        # Agent chooses a random action\n",
    "        action = env.env.action_space.sample()\n",
    "\n",
    "        #  Agent takes the action and get the information from the environment \n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        # Create image frame \n",
    "        img = cv2.resize(state, (160, 210), interpolation=cv2.INTER_CUBIC)\n",
    "        frame = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        image_lst.append(frame)\n",
    "\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "# terminate the environment\n",
    "env.close()\n",
    "\n",
    "# Save gif \n",
    "imageio.mimsave('Media/gym-ex.gif', image_lst, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751bd81",
   "metadata": {},
   "source": [
    "## Applying Reinforcement Learning to Breakout \n",
    "Recall that in Reinforcement Learning then the main goal of our agent is to maximize its expected future reward. This means that at each time step the agent wants to try and select the best action given its current state. \n",
    "\n",
    "However it is not enough for the agent to just select the greedy action at every time-step, the agent must also explore its other options. The agent then 'learns' what actions are good in what states, and 'remembers' these results.\n",
    "\n",
    "So how can we train the agent to play like a human, when all the agent can do is move the paddle and watch what happens? ðŸ¤”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc67205",
   "metadata": {},
   "source": [
    "### The Challenges\n",
    "The first challenge we need to address is that each state of the game contains a lot of information! In this case, each frame of the game is a 210x160 RGB image! That means that the state of the game on any time step will be encoded by 210x160=33600 pixel values... This is to much. But how can we reduce the amount of total information, while keeping important features present? ðŸ¤”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf82bd36",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning\n",
    "RL + Neural Networks = Deep Reinforcement Learning ðŸ¤¯\n",
    "\n",
    "In their 2013 paper by DeepMind [here](https://www.deepmind.com/publications/playing-atari-with-deep-reinforcement-learning), implemented several solutions on how to use neural networks to encode state information. Although our game image is aesthetically appealing to us humans, it is not suitable for the neural network so we will need to modify it. Some of the methods used by Deepmind to pre-prosses the state information are outlined below. ðŸ‘‡\n",
    "\n",
    "**Steps:**\n",
    "1. Rescale to the frame to an 84x84 grayscale image (already close to a 15x reduction in state data)\n",
    "2. We need to also encode the game dynamics because a static image is not enough for the NN to know the direction of the ball. We can encode the dynamics into the game frame by overlaying 4 successive frames \n",
    "\n",
    "The full code used to make the environment optimized for training, is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ebac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(gym_id, seed, keep_rgb=False):\n",
    "    def make_func():\n",
    "        env = gym.make(gym_id)\n",
    "        env = NoopResetEnv(env, noop_max=30)\n",
    "        if keep_rgb:\n",
    "            env = RGBSaveEnv(env)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env = RecordEpisodeStatistics(env)\n",
    "        env = EpisodicLifeEnv(env)\n",
    "        env = FireResetEnv(env)\n",
    "        env = WarpFrame(env)\n",
    "        env = ClipRewardEnv(env)\n",
    "        env = FrameStack(env, 4)\n",
    "        env = ImageToPyTorch(env)\n",
    "\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "    return make_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c0b0d9",
   "metadata": {},
   "source": [
    "Visual representation of what the NN is \"seeing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c738aba5",
   "metadata": {},
   "source": [
    "<img src=\"Media/Transformation.gif\" width=\"300\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d3750f",
   "metadata": {},
   "source": [
    "## How Does The Code Work?  ðŸ’­\n",
    "After our agent selects an action the gym envorment feeds us our 84x84x4 state. This data is then prossesed by 3 convolutional layers (which are good at procsesing images). Lastly these layers are followed by a hidden layer, which is \"connected\" to two outputs \"actor\" and \"critic\". More info on the Actor-Critic Method [here](https://keras.io/examples/rl/actor_critic_cartpole/). This is only a very high level discription of what is going on, and there are alot of steps taken to improve the training of the NN, for more info check out Deepmind's paper [here](https://www.deepmind.com/publications/playing-atari-with-deep-reinforcement-learning). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59fbaf",
   "metadata": {},
   "source": [
    "## How Do I Train My Own model?\n",
    "Make sure you have installed all the dependences outlined in the README.md file then paste the following in your terminal:\n",
    "\n",
    "`python3 ai_breakout_game.py --num-envs=8 --total-timesteps=3000 --multithreading=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a728f",
   "metadata": {},
   "source": [
    "## How Do I Run my model? \n",
    "`python3 ai_breakout_game.py --run=1 --model-filename=\"ai_breakout_game3000.mdl\"`"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5511c47aec90ff6d3bf2282aa911cd8b4e17a7dd8b958807807acb2e44bc0528"
  },
  "kernelspec": {
   "display_name": "Test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
